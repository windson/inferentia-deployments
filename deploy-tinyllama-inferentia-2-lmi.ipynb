{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1159d01",
   "metadata": {},
   "source": [
    "# Deploy TinyLlama Model on AWS Inferentia2 using AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a539d54",
   "metadata": {},
   "source": [
    "We will deploy TinyLlama 1.1B model using DJL Serving for model deployment using AWS LMI Container. We deploy on SageMaker with Inferentia2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89492b2",
   "metadata": {},
   "source": [
    "### 1. Setup SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8678bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662a44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddb7e8",
   "metadata": {},
   "source": [
    "### 2. Prepare Model Serving Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16f5f4",
   "metadata": {},
   "source": [
    "- We use DJL Serving as the model serving framework. DJL Serving needs a configuraiton file *serving.properties* for model deployment. It includes the following configurations for model deployment.\n",
    "  - option.entryPoint: model serving engine\n",
    "  - option.model_id: Hugging Face model tag or s3 path that stores the model\n",
    "  - option.batch_size: model inference batch size\n",
    "  - option.neuron_optimization_level: Neuron compiler optimization level, e.g., 1 for fast compilation and 3 for best performance\n",
    "  - option.tensor_parallel_degree: number of NeuronCores to be used\n",
    "  - option.load_in_8bit: enable/distable int8 weight quantization for reducing memory footprint\n",
    "  - option.n_positions: maximum sequence length\n",
    "  - option.dtype: date type of weight and activation\n",
    "  - option.model_loading_timeout: length of time to timeout in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8ccd7-2c8e-4db9-b049-36cba4ed6d80",
   "metadata": {},
   "source": [
    "- In this example, the model serving framework (DJL Serving) will pull the model from Hugging Face model hub according to *model_id* in the *serving.properties* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e705e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=TinyLlama/TinyLlama-1.1B-Chat-V0.3\n",
    "option.batch_size=1\n",
    "option.neuron_optimize_level=1\n",
    "option.tensor_parallel_degree=2\n",
    "option.load_in_8bit=false\n",
    "option.n_positions=256\n",
    "option.dtype=fp16\n",
    "option.model_loading_timeout=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd8a72",
   "metadata": {},
   "source": [
    "Note: You can also try experimenting with changing values for `option.batch_size=4` with larger batch size and `option.load_in_8bit=True` to enable `int8` weight quantization for model storage in *serving.properties*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe7ed4",
   "metadata": {},
   "source": [
    "- Package the configuration file in a tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c838e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mycode/\n",
      "mycode/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir -p mycode\n",
    "mv serving.properties mycode/\n",
    "tar czvf mycode.tar.gz mycode/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577f823",
   "metadata": {},
   "source": [
    "- Upload the tarball of the model serving artifacts to SageMaker S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758e35eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-474844183433/large-model-lmi/code/mycode.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mycode.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459e360",
   "metadata": {},
   "source": [
    "### 3. Set the DJL-NeuronX container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.inf2.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc786be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-neuronx\", region=region, version=\"0.25.0\",instance_type=instance_type\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce652e19",
   "metadata": {},
   "source": [
    "### 4. Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f0ed9",
   "metadata": {},
   "source": [
    "- Create a SageMaker endpoint of Inferentia2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df2d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e5f3f",
   "metadata": {},
   "source": [
    "- Create a model wrapper that includes docker container and model serving artifacts\n",
    "- Model deployment would take 6~7 minutes as model is compiled during the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8601d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!elapsed time: 302.71287631988525\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "model = Model(image_uri=inference_image_uri, model_data=code_artifact, role=role)\n",
    "model._is_compiled_model = True # let sagemaker know model is compiled as it is done by neuron-cc\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             container_startup_health_check_timeout=900,\n",
    "             volume_size=256,\n",
    "             endpoint_name=endpoint_name)\n",
    "t1 = time.time()\n",
    "print(f\"elapsed time: {t1-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184caba",
   "metadata": {},
   "source": [
    "- Create a predictor for submit inference requests and receive reponses\n",
    "- Requests and responses are in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dac3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe12ab",
   "metadata": {},
   "source": [
    "### 5. Inference test\n",
    "- Submit an inference request to model server and receive inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"tell me a story of the little red riding hood\"]\n",
    "results = predictor.predict(\n",
    "    {\"inputs\": prompts, \"parameters\": {\"max_new_tokens\":256, \"do_sample\":\"true\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a95e0b03-86fb-4db4-b1f5-209195525f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me a story of the little red riding hood. And this is how it happened...\n",
      "The little red riding hood was walking to the forest with her grandmother. The forest was very beautiful, with tall trees and bright flowers.\n",
      "Little Riding Hood heard a rustling sound and turned around. She was surprised to see a huge wolf howling at her. She thought that was only a wild animal.\n",
      "\"You there! You scared me just walk away,\" the wolf growled.\n",
      "Little Riding Hood stepped towards the wolf. She couldn't help but feel a mixture of anger and excitement at her situation.\n",
      "The wolf lunged forward and hit her hard across the face. It was a surprise attack! Little Riding Hood lost her words when she struck the wolf. Blood was streaming down her face.\n",
      "The wolf howled as he realised that little Riding Hood wasn't a threat. He let go of her and continued his way towards home. Little Riding Hood felt a little disappointed, but also relieved.\n",
      "But as she was walking back home, she heard a scream. It was her grandmother'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for result in json.loads(results):\n",
    "    generated_text = result['generated_text']\n",
    "    print(f\"{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f8c45",
   "metadata": {},
   "source": [
    "### 6. Cleanup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56d3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = False\n",
    "if cleanup:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
