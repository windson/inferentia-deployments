{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729a7fc-a1b6-4832-9155-d240ccd8ecc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy LLama2 13B Chat LMI Model on AWS Inferentia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2958bcf-767f-4cd7-826e-963f91565876",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we explore how to host a LLama2 13B Chat large language model on SageMaker using the DeepSpeed. We use DJLServing as the model serving solution in this example that is bundled in the Large Model Inference (LMI) container. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/).\n",
    "\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. \n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy `'meta-llama/Llama-2-13b-chat-hf` model on a `ml.g5.12xlarge` instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae96c9-0255-45fa-bbb3-b5926df154c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisite\n",
    "### Hugging Face Account\n",
    "\n",
    "You need to have Hugging Face account. Sign Up here https://huggingface.co/join with your email if you do not already have account.\n",
    "\n",
    "- For seamless access of the models avaialble on Hugging Face especially gated models such as Llama, for fine-tuning and inferencing purposes, you need to have Hugging Face Account to obtain read Access Token.\n",
    "- After signup, [login](https://huggingface.co/login) to visit https://huggingface.co/settings/tokens to create read Access token.\n",
    "\n",
    "### Request access to the next version of Llama\n",
    "\n",
    "Use the same email id to obtain permission from meta by visiting this link: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "\n",
    "- The Llama models available via Hugging Face are gated models. The use of Llama model is governed by the Meta license. In order to download the model weights and tokenizer, please visit https://ai.meta.com/resources/models-and-libraries/llama-downloads/ and accept their License before requesting access.\n",
    "- Within 2 days you might be granted access to use Llama models via a confirmation email with subject: [Access granted] Your request to access model meta-llama/Llama-2-13b-chat-hf has been accepted. Though the model id is Llama-2-13b-chat-hf, you should be able to access other variants too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6a36f-ee95-453d-a127-c8a7de6a026d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip\n",
    "!pip install -Uq sagemaker boto3 huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0c89f4-679c-4557-b95d-1d954c15a020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d5a162-e9be-469b-910e-18cca8c359f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68b5181-d018-4564-9762-fa8770a9672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_prefix = \"hf-large-model-djl/meta-llama/Llama-2-13b-chat\"\n",
    "s3_code_prefix = f\"{s3_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"{s3_prefix}/model\"  # folder within bucket where model artifact will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e1413-76f8-4b01-88b5-1962c012d438",
   "metadata": {},
   "source": [
    "## Download the model snapshot from Hugging Face and upload the model artifacts on Amazon S3\n",
    "\n",
    "If you intend to download your copy of the model and upload it to a s3 location in your AWS account, please follow the below steps, else you can skip to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d6e18-6760-4f88-835d-150ca372e6eb",
   "metadata": {},
   "source": [
    "Following Snapshot Download will take around 4 to 6 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af859c-4c3a-4fda-ae27-890be565a906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\", \"*.chk\", \"*.pth\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name, \n",
    "    cache_dir=local_model_path, \n",
    "    allow_patterns=allow_patterns, \n",
    "    token='<YOUR_HUGGING_FACE_READ_ACCESS_TOKEN>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8716cd1-1170-4910-a5e9-34c194ac79d8",
   "metadata": {},
   "source": [
    "Upload files to default S3 bucket and obtain the URI in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f355f9f-69e2-4c1e-a467-5c5520a9b142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_s3_uri = sess.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {base_model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "993c024f-c18a-41e2-a784-90629732944e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleanup locally stored model files post S3 upload\n",
    "!rm -rf {model_download_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913de41-193a-4bfb-b42e-581c7677d0ed",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact,  upload Model to S3 and bring your own inference script.\n",
    "\n",
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or postprocessing of the model's predictions.\n",
    "\n",
    "SageMaker needs the model artifacts to be in a Tarball format. In this example, we provide the following files - serving.properties.\n",
    "\n",
    "The tarball is in the following format:\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── serving.properties\n",
    "```\n",
    "\n",
    "    serving.properties is the configuration file that can be used to configure the model server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f5d44-6388-49fa-bc87-2e82bed226f3",
   "metadata": {},
   "source": [
    "### Create serving.properties file for neuronx\n",
    "\n",
    "This is a configuration file to indicate to DJL Serving which model parallelization and inference optimization libraries you would like to use. Depending on your need, you can set the appropriate configuration.\n",
    "\n",
    "Here is a list of settings that we use in this configuration file -\n",
    "\n",
    "- `engine`: The runtime engine for DJL to use. The possible values for engine include *Python*, *DeepSpeed*, *FasterTransformer*, and *MPI*. In this case, we set it to *Python*.\n",
    "- `option.entryPoint`: model serving engine, we will be using *djl_python.transformers_neuronx* for inferentia 2.\n",
    "- `option.model_id`: The model id of a pretrained model hosted inside a [model repository on huggingface](https://huggingface.co/models) or S3 path to the model artefact. \n",
    "- `option.neuron_optimization_level`: Neuron compiler optimization level, e.g., 1 for fast compilation and 3 for best performance\n",
    "- `option.tensor_parallel_degree`: number of NeuronCores to be used\n",
    "- `option.load_in_8bit`: enable/distable int8 weight quantization for reducing memory footprint\n",
    "- `option.n_positions`: maximum sequence length\n",
    "- `option.dtype`: date type of weight and activation\n",
    "- `option.model_loading_timeout`: length of time to timeout in seconds\n",
    "\n",
    "[Amazon EC2 Inf2 Instances](https://aws.amazon.com/ec2/instance-types/inf2/)\n",
    "\n",
    "Since we are serving the model using deepspeed container, and Llama 2 being a large model used for inference,  we are following the approach of [Large model inference with DeepSpeed and DJL Serving](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c2691f5b-4e98-4f7a-887c-49c05bbf7a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf chat_llama2_13b_hf\n",
    "!mkdir -p chat_llama2_13b_hf\n",
    "model_id = base_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6300047f-b377-4448-a446-2cd48121d103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chat_llama2_13b_hf/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile chat_llama2_13b_hf/serving.properties\n",
    "engine = Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id={{model_id}}\n",
    "option.batch_size=8\n",
    "option.neuron_optimize_level=1\n",
    "option.tensor_parallel_degree=12\n",
    "option.load_in_8bit=false\n",
    "option.n_positions=2048\n",
    "option.dtype=fp16\n",
    "option.model_loading_timeout=1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "996ebeb2-cfee-4e7b-af8f-3ccc811fa1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.entryPoint\u001b[39;49;00m=\u001b[33mdjl_python.transformers_neuronx\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33ms3://sagemaker-us-west-2-920487201358/hf-large-model-djl/meta-llama/Llama-2-13b-chat/model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.batch_size\u001b[39;49;00m=\u001b[33m8\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.neuron_optimize_level\u001b[39;49;00m=\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33m12\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     7\t\u001b[36moption.load_in_8bit\u001b[39;49;00m=\u001b[33mfalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     8\t\u001b[36moption.n_positions\u001b[39;49;00m=\u001b[33m2048\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     9\t\u001b[36moption.dtype\u001b[39;49;00m=\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    10\t\u001b[36moption.model_loading_timeout\u001b[39;49;00m=\u001b[33m1500\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties`\n",
    "template = jinja_env.from_string(Path(\"chat_llama2_13b_hf/serving.properties\").open().read())\n",
    "Path(\"chat_llama2_13b_hf/serving.properties\").open(\"w\").write(\n",
    "    template.render(model_id=base_model_s3_uri)\n",
    ")\n",
    "!pygmentize chat_llama2_13b_hf/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db9c4-5023-4125-a413-7c5afa135218",
   "metadata": {},
   "source": [
    "Image URI for the DJL container is being used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8dfabe0a-04f8-486d-94ab-7d6066680954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.25.0-neuronx-sdk2.15.0'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-neuronx\", region=region, version=\"0.25.0\",instance_type=instance_type\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905903de-a4b9-4f41-8cc2-564416ae5d5f",
   "metadata": {},
   "source": [
    "Create the Tarball and then upload to S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c005aa2e-ec1a-4ccf-8f39-67bcbabd0309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'model.tar.gz': No such file or directory\n",
      "chat_llama2_13b_hf/\n",
      "chat_llama2_13b_hf/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz chat_llama2_13b_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bb83ba3b-2ea5-4297-8e85-f16dd4c7c13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9c146caa-735b-433a-a673-89c74733dcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-920487201358/hf-large-model-djl/meta-llama/Llama-2-13b-chat/code/model.tar.gz'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_code_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9dbbb5f3-c1e7-47ee-adcb-b9f8cc4add76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9d3ba-c5a5-4605-9826-bf75dd3c42dc",
   "metadata": {},
   "source": [
    "## Deploy Llama 2 13B Chat LMI Model\n",
    "\n",
    "[Choosing instance types for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-choosing-instance-types.html)\n",
    "\n",
    "We will proceed with deploying `meta-llama/Llama-2-13b-chat-hf` model on `ml.g5.12xlarge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa56b75-356a-4ebf-bebd-7150521c95e9",
   "metadata": {},
   "source": [
    "Steps to deploy the model to SageMaker Endpoint will be as follows:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.inf2.24xlarge\n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 900 to ensure health check starts after the model is ready    \n",
    "3. Create the end point using the endpoint config created    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee8273-8009-4739-af67-79525b6882cd",
   "metadata": {},
   "source": [
    "#### Create the Model\n",
    "Use the image URI for the DJL container and the s3 location to which the tarball was uploaded.\n",
    "\n",
    "The container downloads the model into the `/tmp` space on the instance because SageMaker maps the `/tmp` to the Amazon Elastic Block Store (Amazon EBS) volume that is mounted when we specify the endpoint creation parameter VolumeSizeInGB. \n",
    "It leverages `s5cmd`(https://github.com/peak/s5cmd) which offers a very fast download speed and hence extremely useful when downloading large models.\n",
    "\n",
    "For instances like p4dn, which come pre-built with the volume instance, we can continue to leverage the `/tmp` on the container. The size of this mount is large enough to hold the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b64b5a1a-9fef-4098-8b01-9752c03119eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "endpoint_name = name_from_base(f\"Llama-2-13b-chat-lmi-inf2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da3212ee-1986-4033-a4fa-beb248d5322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------!CPU times: user 251 ms, sys: 20.1 ms, total: 271 ms\n",
      "Wall time: 17min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker import Model\n",
    "model = Model(image_uri=inference_image_uri, model_data=s3_code_artifact, role=role)\n",
    "model._is_compiled_model = True # let sagemaker know model is compiled as it is done by neuron-cc\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             container_startup_health_check_timeout=900,\n",
    "             volume_size=256,\n",
    "             endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ec2d7-6ef8-4383-ac50-6a4984af5c03",
   "metadata": {},
   "source": [
    "#### While you wait for the endpoint to be created, you can read more about:\n",
    "- [Deep Learning containers for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)\n",
    "- [Achieve high performance with lowest cost for generative AI inference using AWS Inferentia2 and AWS Trainium on Amazon SageMaker\n",
    "](https://aws.amazon.com/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aed031-b68c-4e23-8fa7-24483ec189d1",
   "metadata": {},
   "source": [
    "We will store the value of the variable endpoint_name to use it in inference notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "82bca632-7305-4dcb-bf96-ae76337ab53a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n",
      "Stored 'bucket' (str)\n",
      "Stored 's3_prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store \\\n",
    "endpoint_name \\\n",
    "bucket \\\n",
    "s3_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d325031",
   "metadata": {},
   "source": [
    "## Inference Llama 2 13B chat deployed on inf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "55fa97ed-f52f-4456-a214-ca2302f2844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5e872efe-7086-438d-9f21-933961ef0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297fa73-4d6e-4af2-9644-b4b657296e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Write a polite and professional dad joke\",\n",
    "          \"Write a poem on Sliding ice cubes on a pine tree\", ]\n",
    "results = predictor.predict(\n",
    "    {\"inputs\": prompts, \"parameters\": {\"max_new_tokens\":256, \"do_sample\":\"true\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56467b91-0527-40be-9c78-ef7653c40d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a polite and professional dad joke that includes the phrase \"the kids are alright\"\n",
      " and is funny.\n",
      "\n",
      "Here's a sample:\n",
      "\n",
      "\"Hey, did you hear about the latest parenting trend? It's called 'letting the kids be alright.' It's like raising them, but on a beach towel. You just spread them out, let them do their thing, and then go grab a drink. The kids are alright, man!\"\n",
      "\n",
      "This joke is funny because it plays on the idea of \"letting go\" of parental responsibilities and allowing kids to freely express themselves, while also referencing the phrase \"the kids are alright\" to signify that they are doing well. The parenting trend of \"letting the kids be alright\" is a humorous exaggeration of the idea of letting kids be independent and self-sufficient. The punchline of the joke, \"The kids are alright, man!\" adds to the humor by using the colloquial phrase \"man\" to emphasize the message and create a sense of informality and\n",
      "\n",
      "Write a poem on Sliding ice cubes on a pine tree\n",
      "\n",
      "Sliding ice cubes on a pine tree\n",
      "\n",
      "The forest is icy and cold\n",
      "The pine needles rustle of ice,\n",
      "The warm light of the sunset\n",
      "The ice cubes on the pine shine\n",
      "\n",
      "Sliding ice on pine branches\n",
      "Ice blue sparkles in the sky,\n",
      "Christmas eve night, a delight\n",
      "A festive scene in the eye\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for result in json.loads(results):\n",
    "    generated_text = result['generated_text']\n",
    "    print(f\"{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a15a285e-edea-4223-951d-c007d3e864f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup=False\n",
    "if cleanup:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3dec0-6c74-449a-a614-0ac0c82433fc",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [Improve throughput performance of Llama 2 models using Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/improve-throughput-performance-of-llama-2-models-using-amazon-sagemaker/)\n",
    "- [Improve performance of Falcon models with Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/improve-performance-of-falcon-models-with-amazon-sagemaker/)\n",
    "- [serving.properties - Configurations and settings](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html)\n",
    "- [Amazon SageMaker launches a new version of Large Model Inference DLC with TensorRT-LLM support](https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-sagemaker-large-model-inference-dlc-tensorrt-llm-support/)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.r5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
